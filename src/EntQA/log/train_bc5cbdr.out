Namespace(B=4, adam_epsilon=1e-06, add_topic=True, blink=True, cands_embeds_path='/mnt/data/run/BC5CDR/cache_embedding/candidate_embeds.npy', clip=1, data_dir='/mnt/data/Generative-End2End-IE/dataset/BC5CDR/retriever_input/', entity_bsz=2048, epochs=50, fp16=False, fp16_opt_level='O1', gpus='4,5,6,7', gradient_accumulation_steps=2, init_model='/mnt/data/entqa/checkpoints/retriever.pt', k=100, kb_dir='/mnt/data/Generative-End2End-IE/dataset/BC5CDR/kb/', logging_steps=100, lr=2e-06, max_len=128, mention_bsz=4096, model='/mnt/data/run/BC5CDR/retriever.pt', num_cands=64, num_workers=0, out_dir='/mnt/data/run/BC5CDR/reader_input/', pretrained_path='/mnt/data/blink/', rands_ratio=0.9, resume_training=False, seed=42, simpleoptim=False, type_loss='sum_log_nce', use_cached_embeds=False, use_gpu_index=False, use_title=True, warmup_proportion=0.2, weight_decay=0.01)
Using device: cuda
begin loading data
number of entities 268146
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/anaconda/envs/entqa/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Data parallel across 4 GPUs 4,5,6,7
/home/klu/EntQA/data_retriever.py:70: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  labels = np.array(labels)
***** train *****
# train samples: 2598
# val samples: 2599
# test samples: 2744
# epochs: 50
 batch size : 4
 gradient accumulation steps 2
 effective training batch size with accumulation: 8
 # training steps: 16237
 # warmup steps: 3247
 learning rate: 2e-06
 # parameters: 670283776
get candidates embeddings

Epoch 1
mining hard negatives
mining time for epoch   1 are 0:00:18
/anaconda/envs/entqa/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Step        100/16237 | Epoch   1 | Batch   200/  650 | Average Loss  27.6476
Step        200/16237 | Epoch   1 | Batch   400/  650 | Average Loss  26.2520
Step        300/16237 | Epoch   1 | Batch   600/  650 | Average Loss  21.6379
training time for epoch   1 is 0:09:17
Done with epoch   1 | train loss  24.6419 | validation hard recall   0.1135|validation LRAP   0.5803 | validation recall   0.2556| epoch time 0:12:42 
------- new best val perf: -inf --> 0.255625 

Epoch 2
mining hard negatives
mining time for epoch   2 are 0:00:18
/anaconda/envs/entqa/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Step        400/16237 | Epoch   2 | Batch   150/  650 | Average Loss  18.5015
Step        500/16237 | Epoch   2 | Batch   350/  650 | Average Loss  14.3149
Step        600/16237 | Epoch   2 | Batch   550/  650 | Average Loss  12.6157
training time for epoch   2 is 0:09:15
Done with epoch   2 | train loss  19.4442 | validation hard recall   0.1389|validation LRAP   0.4942 | validation recall   0.3418| epoch time 0:12:40 
------- new best val perf: 0.255625 --> 0.341794 

Epoch 3
mining hard negatives
mining time for epoch   3 are 0:00:18
/anaconda/envs/entqa/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Step        700/16237 | Epoch   3 | Batch   100/  650 | Average Loss  11.3927
Step        800/16237 | Epoch   3 | Batch   300/  650 | Average Loss  10.5562
Step        900/16237 | Epoch   3 | Batch   500/  650 | Average Loss  10.5677
training time for epoch   3 is 0:09:14
Done with epoch   3 | train loss  16.5098 | validation hard recall   0.1831|validation LRAP   0.4679 | validation recall   0.4481| epoch time 0:12:39 
------- new best val perf: 0.341794 --> 0.448118 

Epoch 4
mining hard negatives
mining time for epoch   4 are 0:00:18
/anaconda/envs/entqa/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Step       1000/16237 | Epoch   4 | Batch    50/  650 | Average Loss   9.9929
Step       1100/16237 | Epoch   4 | Batch   250/  650 | Average Loss   9.5581
Step       1200/16237 | Epoch   4 | Batch   450/  650 | Average Loss   8.9926
Step       1300/16237 | Epoch   4 | Batch   650/  650 | Average Loss   8.4205
training time for epoch   4 is 0:09:14
Done with epoch   4 | train loss  14.6500 | validation hard recall   0.2374|validation LRAP   0.4620 | validation recall   0.4953| epoch time 0:12:40 
------- new best val perf: 0.448118 --> 0.495254 

Epoch 5
mining hard negatives
mining time for epoch   5 are 0:00:18
/anaconda/envs/entqa/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Step       1400/16237 | Epoch   5 | Batch   200/  650 | Average Loss   8.3573
Step       1500/16237 | Epoch   5 | Batch   400/  650 | Average Loss   7.2912
Step       1600/16237 | Epoch   5 | Batch   600/  650 | Average Loss   7.2922
training time for epoch   5 is 0:09:12
Done with epoch   5 | train loss  13.2427 | validation hard recall   0.2436|validation LRAP   0.4857 | validation recall   0.5223| epoch time 0:12:38 
------- new best val perf: 0.495254 --> 0.522342 

Epoch 6
mining hard negatives
mining time for epoch   6 are 0:00:18
/anaconda/envs/entqa/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Step       1700/16237 | Epoch   6 | Batch   150/  650 | Average Loss   6.9267
Step       1800/16237 | Epoch   6 | Batch   350/  650 | Average Loss   6.3279
Step       1900/16237 | Epoch   6 | Batch   550/  650 | Average Loss   6.2825
training time for epoch   6 is 0:09:10
Done with epoch   6 | train loss  12.0845 | validation hard recall   0.2813|validation LRAP   0.5258 | validation recall   0.5567| epoch time 0:12:35 
------- new best val perf: 0.522342 --> 0.556681 

Epoch 7
mining hard negatives
mining time for epoch   7 are 0:00:18
/anaconda/envs/entqa/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Step       2000/16237 | Epoch   7 | Batch   100/  650 | Average Loss   6.1269
Step       2100/16237 | Epoch   7 | Batch   300/  650 | Average Loss   5.5407
Step       2200/16237 | Epoch   7 | Batch   500/  650 | Average Loss   5.0892
training time for epoch   7 is 0:09:11
Done with epoch   7 | train loss  11.1448 | validation hard recall   0.3178|validation LRAP   0.5082 | validation recall   0.5980| epoch time 0:12:35 
------- new best val perf: 0.556681 --> 0.597952 

Epoch 8
mining hard negatives
mining time for epoch   8 are 0:00:18
/anaconda/envs/entqa/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Step       2300/16237 | Epoch   8 | Batch    50/  650 | Average Loss   5.4371
Step       2400/16237 | Epoch   8 | Batch   250/  650 | Average Loss   4.9837
Step       2500/16237 | Epoch   8 | Batch   450/  650 | Average Loss   4.3850
Step       2600/16237 | Epoch   8 | Batch   650/  650 | Average Loss   4.1913
training time for epoch   8 is 0:09:13
Done with epoch   8 | train loss  10.3339 | validation hard recall   0.3486|validation LRAP   0.5456 | validation recall   0.6320| epoch time 0:12:36 
------- new best val perf: 0.597952 --> 0.631972 

Epoch 9
mining hard negatives
mining time for epoch   9 are 0:00:17
/anaconda/envs/entqa/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Step       2700/16237 | Epoch   9 | Batch   200/  650 | Average Loss   4.9220
Step       2800/16237 | Epoch   9 | Batch   400/  650 | Average Loss   3.9583
Step       2900/16237 | Epoch   9 | Batch   600/  650 | Average Loss   3.7635
training time for epoch   9 is 0:09:11
Done with epoch   9 | train loss   9.6465 | validation hard recall   0.3574|validation LRAP   0.5347 | validation recall   0.6377| epoch time 0:12:35 
------- new best val perf: 0.631972 --> 0.637731 

Epoch 10
mining hard negatives
mining time for epoch  10 are 0:00:17
/anaconda/envs/entqa/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Step       3000/16237 | Epoch  10 | Batch   150/  650 | Average Loss   4.3597
Step       3100/16237 | Epoch  10 | Batch   350/  650 | Average Loss   3.0670
Step       3200/16237 | Epoch  10 | Batch   550/  650 | Average Loss   2.9966
training time for epoch  10 is 0:09:04
Done with epoch  10 | train loss   9.0271 | validation hard recall   0.4152|validation LRAP   0.5994 | validation recall   0.6961| epoch time 0:12:27 
------- new best val perf: 0.637731 --> 0.696065 

Epoch 11
mining hard negatives
mining time for epoch  11 are 0:00:18
/anaconda/envs/entqa/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Step       3300/16237 | Epoch  11 | Batch   100/  650 | Average Loss   3.6374
Step       3400/16237 | Epoch  11 | Batch   300/  650 | Average Loss   3.3281
Step       3500/16237 | Epoch  11 | Batch   500/  650 | Average Loss   2.9647
training time for epoch  11 is 0:09:04
Done with epoch  11 | train loss   8.5076 | validation hard recall   0.4521|validation LRAP   0.5996 | validation recall   0.7316| epoch time 0:12:27 
------- new best val perf: 0.696065 --> 0.731577 

Epoch 12
mining hard negatives
mining time for epoch  12 are 0:00:17
/anaconda/envs/entqa/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Step       3600/16237 | Epoch  12 | Batch    50/  650 | Average Loss   3.5548
Step       3700/16237 | Epoch  12 | Batch   250/  650 | Average Loss   3.0463
Step       3800/16237 | Epoch  12 | Batch   450/  650 | Average Loss   3.0520
Step       3900/16237 | Epoch  12 | Batch   650/  650 | Average Loss   2.7690
training time for epoch  12 is 0:09:04
Done with epoch  12 | train loss   8.0539 | validation hard recall   0.4871|validation LRAP   0.6428 | validation recall   0.7582| epoch time 0:12:26 
------- new best val perf: 0.731577 --> 0.758238 

Epoch 13
mining hard negatives
mining time for epoch  13 are 0:00:18
/anaconda/envs/entqa/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Step       4000/16237 | Epoch  13 | Batch   200/  650 | Average Loss   2.8505
Step       4100/16237 | Epoch  13 | Batch   400/  650 | Average Loss   2.9590
Step       4200/16237 | Epoch  13 | Batch   600/  650 | Average Loss   2.5211
training time for epoch  13 is 0:09:03
Done with epoch  13 | train loss   7.6461 | validation hard recall   0.5221|validation LRAP   0.6571 | validation recall   0.7854| epoch time 0:12:25 
------- new best val perf: 0.758238 --> 0.785432 

Epoch 14
mining hard negatives
mining time for epoch  14 are 0:00:17
/anaconda/envs/entqa/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Step       4300/16237 | Epoch  14 | Batch   150/  650 | Average Loss   2.5193
Step       4400/16237 | Epoch  14 | Batch   350/  650 | Average Loss   2.4645
Step       4500/16237 | Epoch  14 | Batch   550/  650 | Average Loss   2.2379
training time for epoch  14 is 0:09:05
Done with epoch  14 | train loss   7.2682 | validation hard recall   0.5260|validation LRAP   0.6631 | validation recall   0.7901| epoch time 0:12:28 
------- new best val perf: 0.785432 --> 0.790125 

Epoch 15
mining hard negatives
mining time for epoch  15 are 0:00:17
/anaconda/envs/entqa/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Step       4600/16237 | Epoch  15 | Batch   100/  650 | Average Loss   2.0894
Step       4700/16237 | Epoch  15 | Batch   300/  650 | Average Loss   2.0995
Step       4800/16237 | Epoch  15 | Batch   500/  650 | Average Loss   2.0943
training time for epoch  15 is 0:09:05
Done with epoch  15 | train loss   6.9248 | validation hard recall   0.5341|validation LRAP   0.6852 | validation recall   0.7965| epoch time 0:12:28 
------- new best val perf: 0.790125 --> 0.796523 

Epoch 16
mining hard negatives
mining time for epoch  16 are 0:00:17
/anaconda/envs/entqa/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Step       4900/16237 | Epoch  16 | Batch    50/  650 | Average Loss   2.1332
Step       5000/16237 | Epoch  16 | Batch   250/  650 | Average Loss   1.9862
Step       5100/16237 | Epoch  16 | Batch   450/  650 | Average Loss   1.8414
Step       5200/16237 | Epoch  16 | Batch   650/  650 | Average Loss   1.7795
training time for epoch  16 is 0:09:05
Done with epoch  16 | train loss   6.6092 | validation hard recall   0.5433|validation LRAP   0.6858 | validation recall   0.8011| epoch time 0:12:27 
------- new best val perf: 0.796523 --> 0.801109 

Epoch 17
mining hard negatives
mining time for epoch  17 are 0:00:18
/anaconda/envs/entqa/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Step       5300/16237 | Epoch  17 | Batch   200/  650 | Average Loss   1.9276
Step       5400/16237 | Epoch  17 | Batch   400/  650 | Average Loss   1.6333
Step       5500/16237 | Epoch  17 | Batch   600/  650 | Average Loss   1.5945
training time for epoch  17 is 0:09:05
Done with epoch  17 | train loss   6.3199 | validation hard recall   0.5487|validation LRAP   0.6867 | validation recall   0.8035| epoch time 0:12:28 
------- new best val perf: 0.801109 --> 0.803455 

Epoch 18
mining hard negatives
mining time for epoch  18 are 0:00:18
/anaconda/envs/entqa/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Step       5600/16237 | Epoch  18 | Batch   150/  650 | Average Loss   1.2754
Step       5700/16237 | Epoch  18 | Batch   350/  650 | Average Loss   1.3211
Step       5800/16237 | Epoch  18 | Batch   550/  650 | Average Loss   1.7335
training time for epoch  18 is 0:09:04
Done with epoch  18 | train loss   6.0490 | validation hard recall   0.5498|validation LRAP   0.6920 | validation recall   0.8035| epoch time 0:12:27 
------- new best val perf: 0.803455 --> 0.803455 

Epoch 19
mining hard negatives
mining time for epoch  19 are 0:00:17
/anaconda/envs/entqa/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Step       5900/16237 | Epoch  19 | Batch   100/  650 | Average Loss   1.3173
Step       6000/16237 | Epoch  19 | Batch   300/  650 | Average Loss   1.3804
Step       6100/16237 | Epoch  19 | Batch   500/  650 | Average Loss   1.1705
training time for epoch  19 is 0:09:03
Done with epoch  19 | train loss   5.7987 | validation hard recall   0.5475|validation LRAP   0.6987 | validation recall   0.8044| epoch time 0:12:25 
------- new best val perf: 0.803455 --> 0.804415 

Epoch 20
mining hard negatives
mining time for epoch  20 are 0:00:18
/anaconda/envs/entqa/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Step       6200/16237 | Epoch  20 | Batch    50/  650 | Average Loss   1.3609
Step       6300/16237 | Epoch  20 | Batch   250/  650 | Average Loss   1.0569
Step       6400/16237 | Epoch  20 | Batch   450/  650 | Average Loss   1.1115
Step       6500/16237 | Epoch  20 | Batch   650/  650 | Average Loss   1.0138
training time for epoch  20 is 0:09:05
Done with epoch  20 | train loss   5.5627 | validation hard recall   0.5560|validation LRAP   0.6938 | validation recall   0.8078| epoch time 0:12:28 
------- new best val perf: 0.804415 --> 0.807828 

Epoch 21
mining hard negatives
mining time for epoch  21 are 0:00:17
/anaconda/envs/entqa/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Step       6600/16237 | Epoch  21 | Batch   200/  650 | Average Loss   1.1233
Step       6700/16237 | Epoch  21 | Batch   400/  650 | Average Loss   1.0425
Step       6800/16237 | Epoch  21 | Batch   600/  650 | Average Loss   0.9718
training time for epoch  21 is 0:09:06
Done with epoch  21 | train loss   5.3467 | validation hard recall   0.5568|validation LRAP   0.7086 | validation recall   0.8080| epoch time 0:12:29 
------- new best val perf: 0.807828 --> 0.808041 

Epoch 22
mining hard negatives
mining time for epoch  22 are 0:00:17
/anaconda/envs/entqa/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Step       6900/16237 | Epoch  22 | Batch   150/  650 | Average Loss   0.9580
Step       7000/16237 | Epoch  22 | Batch   350/  650 | Average Loss   0.9251
Step       7100/16237 | Epoch  22 | Batch   550/  650 | Average Loss   0.9877
training time for epoch  22 is 0:09:04
Done with epoch  22 | train loss   5.1488 | validation hard recall   0.5556|validation LRAP   0.7115 | validation recall   0.8071| epoch time 0:12:27 


Epoch 23
mining hard negatives
mining time for epoch  23 are 0:00:18
/anaconda/envs/entqa/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Step       7200/16237 | Epoch  23 | Batch   100/  650 | Average Loss   1.0099
Step       7300/16237 | Epoch  23 | Batch   300/  650 | Average Loss   1.1005
Step       7400/16237 | Epoch  23 | Batch   500/  650 | Average Loss   0.8998
training time for epoch  23 is 0:09:05
Done with epoch  23 | train loss   4.9686 | validation hard recall   0.5610|validation LRAP   0.7094 | validation recall   0.8129| epoch time 0:12:28 
------- new best val perf: 0.808041 --> 0.812947 

Epoch 24
mining hard negatives
mining time for epoch  24 are 0:00:18
/anaconda/envs/entqa/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Step       7500/16237 | Epoch  24 | Batch    50/  650 | Average Loss   0.9878
Step       7600/16237 | Epoch  24 | Batch   250/  650 | Average Loss   0.8238
Step       7700/16237 | Epoch  24 | Batch   450/  650 | Average Loss   0.6958
Step       7800/16237 | Epoch  24 | Batch   650/  650 | Average Loss   0.7843
training time for epoch  24 is 0:09:04
Done with epoch  24 | train loss   4.7934 | validation hard recall   0.5529|validation LRAP   0.7121 | validation recall   0.8081| epoch time 0:12:28 


Epoch 25
mining hard negatives
mining time for epoch  25 are 0:00:18
/anaconda/envs/entqa/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Step       7900/16237 | Epoch  25 | Batch   200/  650 | Average Loss   0.7468
Step       8000/16237 | Epoch  25 | Batch   400/  650 | Average Loss   0.7259
Step       8100/16237 | Epoch  25 | Batch   600/  650 | Average Loss   0.6638
training time for epoch  25 is 0:09:04
Done with epoch  25 | train loss   4.6305 | validation hard recall   0.5514|validation LRAP   0.7154 | validation recall   0.8039| epoch time 0:12:28 


Epoch 26
mining hard negatives
mining time for epoch  26 are 0:00:17
/anaconda/envs/entqa/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Step       8200/16237 | Epoch  26 | Batch   150/  650 | Average Loss   0.6709
Step       8300/16237 | Epoch  26 | Batch   350/  650 | Average Loss   0.7882
Step       8400/16237 | Epoch  26 | Batch   550/  650 | Average Loss   0.6785
training time for epoch  26 is 0:09:06
Done with epoch  26 | train loss   4.4790 | validation hard recall   0.5618|validation LRAP   0.7171 | validation recall   0.8110| epoch time 0:12:29 


Epoch 27
mining hard negatives
mining time for epoch  27 are 0:00:17
/anaconda/envs/entqa/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Step       8500/16237 | Epoch  27 | Batch   100/  650 | Average Loss   0.6011
Step       8600/16237 | Epoch  27 | Batch   300/  650 | Average Loss   0.5334
Step       8700/16237 | Epoch  27 | Batch   500/  650 | Average Loss   0.7052
training time for epoch  27 is 0:09:05
Done with epoch  27 | train loss   4.3372 | validation hard recall   0.5583|validation LRAP   0.7194 | validation recall   0.8089| epoch time 0:12:28 


Epoch 28
mining hard negatives
mining time for epoch  28 are 0:00:17
/anaconda/envs/entqa/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Step       8800/16237 | Epoch  28 | Batch    50/  650 | Average Loss   0.7206
Step       8900/16237 | Epoch  28 | Batch   250/  650 | Average Loss   0.6355
Step       9000/16237 | Epoch  28 | Batch   450/  650 | Average Loss   0.6134
Step       9100/16237 | Epoch  28 | Batch   650/  650 | Average Loss   0.6788
training time for epoch  28 is 0:09:09
Done with epoch  28 | train loss   4.2049 | validation hard recall   0.5548|validation LRAP   0.7184 | validation recall   0.8054| epoch time 0:13:29 


Epoch 29
mining hard negatives
mining time for epoch  29 are 0:00:18
/anaconda/envs/entqa/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Step       9200/16237 | Epoch  29 | Batch   200/  650 | Average Loss   0.5178
Step       9300/16237 | Epoch  29 | Batch   400/  650 | Average Loss   0.6490
Step       9400/16237 | Epoch  29 | Batch   600/  650 | Average Loss   0.4910
training time for epoch  29 is 0:09:05
Done with epoch  29 | train loss   4.0792 | validation hard recall   0.5610|validation LRAP   0.7198 | validation recall   0.8100| epoch time 0:12:28 


Epoch 30
mining hard negatives
mining time for epoch  30 are 0:00:18
/anaconda/envs/entqa/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Step       9500/16237 | Epoch  30 | Batch   150/  650 | Average Loss   0.5186
Step       9600/16237 | Epoch  30 | Batch   350/  650 | Average Loss   0.6178
Step       9700/16237 | Epoch  30 | Batch   550/  650 | Average Loss   0.5158
training time for epoch  30 is 0:09:05
Done with epoch  30 | train loss   3.9611 | validation hard recall   0.5579|validation LRAP   0.7171 | validation recall   0.8102| epoch time 0:12:28 


Epoch 31
mining hard negatives
mining time for epoch  31 are 0:00:18
/anaconda/envs/entqa/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Step       9800/16237 | Epoch  31 | Batch   100/  650 | Average Loss   0.5530
Step       9900/16237 | Epoch  31 | Batch   300/  650 | Average Loss   0.5816
Step      10000/16237 | Epoch  31 | Batch   500/  650 | Average Loss   0.3676
training time for epoch  31 is 0:09:04
Done with epoch  31 | train loss   3.8491 | validation hard recall   0.5625|validation LRAP   0.7213 | validation recall   0.8104| epoch time 0:12:27 


Epoch 32
mining hard negatives
mining time for epoch  32 are 0:00:18
/anaconda/envs/entqa/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Step      10100/16237 | Epoch  32 | Batch    50/  650 | Average Loss   0.4581
Step      10200/16237 | Epoch  32 | Batch   250/  650 | Average Loss   0.4481
Step      10300/16237 | Epoch  32 | Batch   450/  650 | Average Loss   0.4184
Step      10400/16237 | Epoch  32 | Batch   650/  650 | Average Loss   0.5403
training time for epoch  32 is 0:09:04
Done with epoch  32 | train loss   3.7435 | validation hard recall   0.5575|validation LRAP   0.7251 | validation recall   0.8079| epoch time 0:12:28 


Epoch 33
mining hard negatives
mining time for epoch  33 are 0:00:18
/anaconda/envs/entqa/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Step      10500/16237 | Epoch  33 | Batch   200/  650 | Average Loss   0.5374
Step      10600/16237 | Epoch  33 | Batch   400/  650 | Average Loss   0.3487
Step      10700/16237 | Epoch  33 | Batch   600/  650 | Average Loss   0.3492
training time for epoch  33 is 0:09:04
Done with epoch  33 | train loss   3.6427 | validation hard recall   0.5606|validation LRAP   0.7243 | validation recall   0.8110| epoch time 0:12:28 


Epoch 34
mining hard negatives
mining time for epoch  34 are 0:00:18
/anaconda/envs/entqa/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Step      10800/16237 | Epoch  34 | Batch   150/  650 | Average Loss   0.3676
Step      10900/16237 | Epoch  34 | Batch   350/  650 | Average Loss   0.4726
Step      11000/16237 | Epoch  34 | Batch   550/  650 | Average Loss   0.3600
training time for epoch  34 is 0:09:04
Done with epoch  34 | train loss   3.5470 | validation hard recall   0.5587|validation LRAP   0.7286 | validation recall   0.8107| epoch time 0:12:28 


Epoch 35
mining hard negatives
mining time for epoch  35 are 0:00:18
/anaconda/envs/entqa/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Step      11100/16237 | Epoch  35 | Batch   100/  650 | Average Loss   0.3982
Step      11200/16237 | Epoch  35 | Batch   300/  650 | Average Loss   0.4178
Step      11300/16237 | Epoch  35 | Batch   500/  650 | Average Loss   0.3717
training time for epoch  35 is 0:09:04
Done with epoch  35 | train loss   3.4572 | validation hard recall   0.5614|validation LRAP   0.7227 | validation recall   0.8133| epoch time 0:12:28 
------- new best val perf: 0.812947 --> 0.813267 

Epoch 36
mining hard negatives
mining time for epoch  36 are 0:00:18
/anaconda/envs/entqa/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Step      11400/16237 | Epoch  36 | Batch    50/  650 | Average Loss   0.4337
Step      11500/16237 | Epoch  36 | Batch   250/  650 | Average Loss   0.3368
Step      11600/16237 | Epoch  36 | Batch   450/  650 | Average Loss   0.3953
Step      11700/16237 | Epoch  36 | Batch   650/  650 | Average Loss   0.4352
training time for epoch  36 is 0:09:03
Done with epoch  36 | train loss   3.3722 | validation hard recall   0.5648|validation LRAP   0.7249 | validation recall   0.8151| epoch time 0:12:26 
------- new best val perf: 0.813267 --> 0.815079 

Epoch 37
mining hard negatives
mining time for epoch  37 are 0:00:18
/anaconda/envs/entqa/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Step      11800/16237 | Epoch  37 | Batch   200/  650 | Average Loss   0.4045
Step      11900/16237 | Epoch  37 | Batch   400/  650 | Average Loss   0.2711
Step      12000/16237 | Epoch  37 | Batch   600/  650 | Average Loss   0.4287
training time for epoch  37 is 0:09:04
Done with epoch  37 | train loss   3.2910 | validation hard recall   0.5641|validation LRAP   0.7254 | validation recall   0.8148| epoch time 0:12:30 


Epoch 38
mining hard negatives
mining time for epoch  38 are 0:00:18
/anaconda/envs/entqa/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Step      12100/16237 | Epoch  38 | Batch   150/  650 | Average Loss   0.5076
Step      12200/16237 | Epoch  38 | Batch   350/  650 | Average Loss   0.3397
Step      12300/16237 | Epoch  38 | Batch   550/  650 | Average Loss   0.3485
training time for epoch  38 is 0:09:05
Done with epoch  38 | train loss   3.2148 | validation hard recall   0.5668|validation LRAP   0.7268 | validation recall   0.8131| epoch time 0:12:29 


Epoch 39
mining hard negatives
mining time for epoch  39 are 0:00:18
/anaconda/envs/entqa/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Step      12400/16237 | Epoch  39 | Batch   100/  650 | Average Loss   0.3485
Step      12500/16237 | Epoch  39 | Batch   300/  650 | Average Loss   0.3137
Step      12600/16237 | Epoch  39 | Batch   500/  650 | Average Loss   0.3853
training time for epoch  39 is 0:09:06
Done with epoch  39 | train loss   3.1414 | validation hard recall   0.5621|validation LRAP   0.7274 | validation recall   0.8127| epoch time 0:12:29 


Epoch 40
mining hard negatives
mining time for epoch  40 are 0:00:17
/anaconda/envs/entqa/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Step      12700/16237 | Epoch  40 | Batch    50/  650 | Average Loss   0.3532
Step      12800/16237 | Epoch  40 | Batch   250/  650 | Average Loss   0.3447
Step      12900/16237 | Epoch  40 | Batch   450/  650 | Average Loss   0.3802
Step      13000/16237 | Epoch  40 | Batch   650/  650 | Average Loss   0.4629
training time for epoch  40 is 0:09:05
Done with epoch  40 | train loss   3.0726 | validation hard recall   0.5564|validation LRAP   0.7274 | validation recall   0.8121| epoch time 0:12:28 


Epoch 41
mining hard negatives
mining time for epoch  41 are 0:00:18
/anaconda/envs/entqa/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Step      13100/16237 | Epoch  41 | Batch   200/  650 | Average Loss   0.2924
Step      13200/16237 | Epoch  41 | Batch   400/  650 | Average Loss   0.3944
Step      13300/16237 | Epoch  41 | Batch   600/  650 | Average Loss   0.2446
training time for epoch  41 is 0:09:07
Done with epoch  41 | train loss   3.0053 | validation hard recall   0.5618|validation LRAP   0.7301 | validation recall   0.8133| epoch time 0:12:31 


Epoch 42
mining hard negatives
mining time for epoch  42 are 0:00:17
/anaconda/envs/entqa/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Step      13400/16237 | Epoch  42 | Batch   150/  650 | Average Loss   0.3711
Step      13500/16237 | Epoch  42 | Batch   350/  650 | Average Loss   0.3629
Step      13600/16237 | Epoch  42 | Batch   550/  650 | Average Loss   0.2373
training time for epoch  42 is 0:09:06
Done with epoch  42 | train loss   2.9413 | validation hard recall   0.5633|validation LRAP   0.7329 | validation recall   0.8148| epoch time 0:12:29 


Epoch 43
mining hard negatives
mining time for epoch  43 are 0:00:17
/anaconda/envs/entqa/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Step      13700/16237 | Epoch  43 | Batch   100/  650 | Average Loss   0.2688
Step      13800/16237 | Epoch  43 | Batch   300/  650 | Average Loss   0.2837
Step      13900/16237 | Epoch  43 | Batch   500/  650 | Average Loss   0.3493
training time for epoch  43 is 0:09:05
Done with epoch  43 | train loss   2.8802 | validation hard recall   0.5633|validation LRAP   0.7314 | validation recall   0.8137| epoch time 0:12:28 


Epoch 44
mining hard negatives
mining time for epoch  44 are 0:00:18
/anaconda/envs/entqa/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Step      14000/16237 | Epoch  44 | Batch    50/  650 | Average Loss   0.3494
Step      14100/16237 | Epoch  44 | Batch   250/  650 | Average Loss   0.3446
Step      14200/16237 | Epoch  44 | Batch   450/  650 | Average Loss   0.3058
Step      14300/16237 | Epoch  44 | Batch   650/  650 | Average Loss   0.2944
training time for epoch  44 is 0:09:05
Done with epoch  44 | train loss   2.8219 | validation hard recall   0.5652|validation LRAP   0.7328 | validation recall   0.8144| epoch time 0:12:28 


Epoch 45
mining hard negatives
mining time for epoch  45 are 0:00:18
/anaconda/envs/entqa/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Step      14400/16237 | Epoch  45 | Batch   200/  650 | Average Loss   0.2962
Step      14500/16237 | Epoch  45 | Batch   400/  650 | Average Loss   0.3094
Step      14600/16237 | Epoch  45 | Batch   600/  650 | Average Loss   0.2832
training time for epoch  45 is 0:09:04
Done with epoch  45 | train loss   2.7660 | validation hard recall   0.5648|validation LRAP   0.7308 | validation recall   0.8147| epoch time 0:12:27 


Epoch 46
mining hard negatives
mining time for epoch  46 are 0:00:17
/anaconda/envs/entqa/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Step      14700/16237 | Epoch  46 | Batch   150/  650 | Average Loss   0.3959
Step      14800/16237 | Epoch  46 | Batch   350/  650 | Average Loss   0.3424
Step      14900/16237 | Epoch  46 | Batch   550/  650 | Average Loss   0.3922
training time for epoch  46 is 0:09:04
Done with epoch  46 | train loss   2.7138 | validation hard recall   0.5648|validation LRAP   0.7295 | validation recall   0.8148| epoch time 0:12:27 


Epoch 47
mining hard negatives
mining time for epoch  47 are 0:00:18
/anaconda/envs/entqa/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Step      15000/16237 | Epoch  47 | Batch   100/  650 | Average Loss   0.3499
Step      15100/16237 | Epoch  47 | Batch   300/  650 | Average Loss   0.3548
Step      15200/16237 | Epoch  47 | Batch   500/  650 | Average Loss   0.2459
training time for epoch  47 is 0:09:05
Done with epoch  47 | train loss   2.6624 | validation hard recall   0.5625|validation LRAP   0.7332 | validation recall   0.8144| epoch time 0:12:28 


Epoch 48
mining hard negatives
mining time for epoch  48 are 0:00:18
/anaconda/envs/entqa/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Step      15300/16237 | Epoch  48 | Batch    50/  650 | Average Loss   0.2397
Step      15400/16237 | Epoch  48 | Batch   250/  650 | Average Loss   0.3371
Step      15500/16237 | Epoch  48 | Batch   450/  650 | Average Loss   0.2898
Step      15600/16237 | Epoch  48 | Batch   650/  650 | Average Loss   0.2264
training time for epoch  48 is 0:09:04
Done with epoch  48 | train loss   2.6128 | validation hard recall   0.5652|validation LRAP   0.7319 | validation recall   0.8150| epoch time 0:12:27 


Epoch 49
mining hard negatives
mining time for epoch  49 are 0:00:18
/anaconda/envs/entqa/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Step      15700/16237 | Epoch  49 | Batch   200/  650 | Average Loss   0.3024
Step      15800/16237 | Epoch  49 | Batch   400/  650 | Average Loss   0.2201
Step      15900/16237 | Epoch  49 | Batch   600/  650 | Average Loss   0.2217
training time for epoch  49 is 0:09:06
Done with epoch  49 | train loss   2.5644 | validation hard recall   0.5656|validation LRAP   0.7316 | validation recall   0.8154| epoch time 0:12:29 
------- new best val perf: 0.815079 --> 0.815399 

Epoch 50
mining hard negatives
mining time for epoch  50 are 0:00:18
/anaconda/envs/entqa/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Step      16000/16237 | Epoch  50 | Batch   150/  650 | Average Loss   0.2289
Step      16100/16237 | Epoch  50 | Batch   350/  650 | Average Loss   0.2408
Step      16200/16237 | Epoch  50 | Batch   550/  650 | Average Loss   0.3089
training time for epoch  50 is 0:09:06
Done with epoch  50 | train loss   2.5184 | validation hard recall   0.5648|validation LRAP   0.7315 | validation recall   0.8154| epoch time 0:12:29 
------- new best val perf: 0.815399 --> 0.815399 
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Data parallel across 4 GPUs 4,5,6,7
getting test mention embeddings ...
test inference time 0:00:19
 test hard recall@100 :   0.5448| test LRAP :   0.7175| test recall :   0.8032| 
saving test pairs
val inference time 0:00:18 |val infer time per instance 0:00:00.006978
 val hard recall@100 :   0.5648| val LRAP :   0.7315| val recall :   0.8154| 
saving val pairs
saving train pairs
experiments time 10:36:06
